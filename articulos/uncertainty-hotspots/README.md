# Uncertainty in Crime Hotspot Prediction

This repository provides a complete experimental framework for evaluating crime hotspot prediction under uncertainty, combining synthetic simulations and real-world data from the city of Chicago.

---

## ğŸš© Research Objective

Our main goal is to evaluate **predictive performance and uncertainty quantification** in spatial-temporal crime forecasting. We aim to support **risk-aware interventions** by producing interpretable metrics like **confidence**, **coverage**, and **priority scores** at the cell level.

---

## ğŸ“‚ Repository Structure

```
uncertainty-hotspots
â”œâ”€â”€ config.json                  # Global configuration
â”œâ”€â”€ data/                        # Local processed datasets (real and synthetic)
â”‚   â””â”€â”€ real_data/Chicago/       # Preprocessed real crime data and metadata
â”‚   â””â”€â”€ examples/                # Single example dataset for quick reference
â”œâ”€â”€ results/                     # Saved models, metrics, visualizations
â”œâ”€â”€ notebooks/                   # Jupyter notebooks for each experiment
â””â”€â”€ experiments/                 # Custom or extended experiments
```

> ğŸ” **External Data** (not included): Real data is stored in  
> `../uncertainty-informed-data/real_data/Chicago/`.
> `../uncertainty-informed-data/simulations/poisson/`.

---

## ğŸ§  General Workflow Overview

### 1. âš™ï¸ Configuration Setup

- Global parameters are defined in `config.json`:
  - Grid size, partitions (train/calibration/test), hotspot definitions, etc.
  - Used consistently across synthetic and real-world pipelines.

---

## ğŸ”¬ Experiments Overview

### ğŸ“ `01_preprocessing_real_data.ipynb`

- Loads and cleans the official 2024 Chicago crime dataset.
- Selects relevant crime types (e.g., `ASSAULT`, `ROBBERY`, `NARCOTICS`).
- Maps police beats to a 2D grid using a reproducible spatial mapping.
- Aggregates daily counts and exports a formatted dataset.

### ğŸ§ª `02_model_naive_evaluation.ipynb`

- Trains a naive per-cell model (mean count) on the Chicago dataset.
- Evaluates it on the test set using traditional spatial metrics:
  - RMSE, MAE, PAI, PEI, PEI*
- Produces baseline comparisons across different hotspot coverage levels.

### ğŸ“Š `03_visual_comparison_predictions.ipynb`

- Visualizes daily and average predictions vs. ground truth.
- Highlights spatial variability in hotspot coverage.
- Saves prediction masks and hotspots for future comparison.

### ğŸ“ `04_conformal_prediction_analysis.ipynb`

- Applies **MAPIE** (Conformal Prediction) to the naive model.
- Computes per-cell prediction intervals.
- Measures and visualizes:
  - Interval width
  - Misscoverage rate
  - Confidence scores
- Introduces a **Hotspot Priority Map**:
  - Combines confidence and frequency into a **4-class taxonomy**.

| Category        | Frequency | Confidence | Color    |
|----------------|-----------|-------------|----------|
| ğŸŸ¥ Priority     | High      | High        | Red      |
| ğŸŸ§ Critical     | High      | Low         | Orange   |
| ğŸŸ¨ Monitoring   | Low       | Low         | Yellow   |
| ğŸŸ© Low Interest | Low       | High        | Green    |

- Visualizes sensitivity to confidence and frequency thresholds.
- Stores intermediate results for further exploration.

### ğŸ§ª `05_evaluation_new_hotspots.ipynb`

- Compares baseline hotspot predictions with **new prioritized hotspots**.
- Computes PEI\* across multiple scenarios:
  - Ground truth historical frequency
  - Binary hotspot mask
  - Continuous predicted intensity
- Evaluates **sensitivity** of performance to thresholds in:
  - Confidence
  - Risk frequency
- Produces visual summaries of PEI* trends under various configurations.

---

## ğŸ“¦ Key Modules (src/)

- `models/`: naive, poisson models per cell
- `evaluation/`: metrics for CP, PAI/PEI, temporal/spatial evaluation
- `conformal/`: wrapper for MAPIE and per-cell calibration
- `utils/`: preprocessing, grid transforms, plotting

---

## ğŸ” Uncertainty-Aware Hotspot Prioritization

We define **spatio-temporal confidence** per cell:

```
Confidence(t,r,c) = 1 âˆ’ NormalizedIntervalWidth(t,r,c)
```

This score enables **real-time prioritization**, even without future labels.

Using both **confidence** and **frequency**, cells are classified into four categories that support operational decision-making.

---

## ğŸ“ˆ Metrics

We evaluate:

- **Per-day RMSE/MAE** with std. deviation
- **PAI**, **PEI**, and **PEI*** with varying coverage thresholds
- **Misscoverage** and **Interval Width** per cell
- **Sensitivity to threshold values** in priority mapping

---

## ğŸ“– References

- **MAPIE:** [https://github.com/scikit-learn-contrib/MAPIE](https://github.com/scikit-learn-contrib/MAPIE)  
- Shafer & Vovk (2008): ["A tutorial on conformal prediction"](https://www.jmlr.org/papers/v9/shafer08a.html)

---

## ğŸš§ Limitations and Future Work

- Assumes i.i.d. (CP requirement), doesn't model cascading events (e.g., SEPP).
- Hotspot allocation uses fixed percentage; dynamic patrol simulation is future work.
- Real data uses uniform grid over beats; no population normalization yet.

---

## âœ… Status

âœ”ï¸ Full pipeline implemented for real data  
âœ”ï¸ Modular structure, reproducible experiments  
âœ”ï¸ Integration of conformal prediction and risk-based prioritization  
â¬œ Integration with additional ML models  
â¬œ Longitudinal drift evaluation  
â¬œ Deployment-ready dashboard